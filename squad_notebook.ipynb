{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squad Pipeline for Question Answering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install necessary packages and restart de kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip requirements\n",
    "!python -m pip install --user --upgrade pip\n",
    "!pip install --user --upgrade kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow SDK\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import OutputPath, InputPath, func_to_container_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(dataset_path: OutputPath(str)):\n",
    "    import json\n",
    "    import os\n",
    "    import tempfile\n",
    "    import zipfile\n",
    "\n",
    "    import requests\n",
    "\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Download GloVe\n",
    "    print(\"Downloading glove\")\n",
    "    GLOVE_DIR = dataset_path + \"/data/glove\"\n",
    "    os.makedirs(GLOVE_DIR, exist_ok=True)\n",
    "    r = requests.get(\"http://nlp.stanford.edu/data/glove.6B.zip\", stream=True)\n",
    "    total_size_in_bytes = int(r.headers.get(\"content-length\", 0))\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n",
    "    with tempfile.TemporaryFile() as tf:\n",
    "        for chunk in r.iter_content(chunk_size=1024):\n",
    "            progress_bar.update(len(chunk))\n",
    "            tf.write(chunk)\n",
    "        with zipfile.ZipFile(tf, \"r\") as f:\n",
    "            f.extractall(GLOVE_DIR)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "\n",
    "    print(\"Finished downloading glove\")\n",
    "\n",
    "    # Download SQuAD\n",
    "    SQUAD_DIR = dataset_path + \"/data/squad\"\n",
    "    os.makedirs(SQUAD_DIR, exist_ok=True)\n",
    "    r_train = requests.get(\n",
    "        \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"\n",
    "    )\n",
    "    squad_train_json = json.loads(r_train.text)\n",
    "    with open(SQUAD_DIR + \"/train-v1.1.json\", \"w\") as f:\n",
    "        json.dump(squad_train_json, f)\n",
    "    print(os.listdir(SQUAD_DIR))\n",
    "\n",
    "    r_dev = requests.get(\n",
    "        \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
    "    )\n",
    "    squad_dev_json = json.loads(r_dev.text)\n",
    "    with open(SQUAD_DIR + \"/dev-v1.1.json\", \"w\") as f:\n",
    "        json.dump(squad_dev_json, f)\n",
    "    print(os.listdir(SQUAD_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess squad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_basic(\n",
    "    dataset_path: InputPath(str),\n",
    "    train_ratio,\n",
    "    glove_vec_size,\n",
    "    mode,\n",
    "    tokenizer,\n",
    "    url,\n",
    "    port,\n",
    "    prepro_squad_dir: OutputPath(str),\n",
    "):\n",
    "    import nltk\n",
    "\n",
    "    from squad.prepro import prepro\n",
    "\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "    train_ratio = float(train_ratio)\n",
    "    glove_vec_size = int(glove_vec_size)\n",
    "    port = int(port)\n",
    "\n",
    "    def main():\n",
    "        args = get_args()\n",
    "        prepro(args)\n",
    "\n",
    "    def get_args():\n",
    "        source_dir = dataset_path + \"/data/squad\"\n",
    "        target_dir = prepro_squad_dir + \"/squad\"\n",
    "        glove_dir = dataset_path + \"/data/glove\"\n",
    "        from types import SimpleNamespace\n",
    "\n",
    "        args = SimpleNamespace(\n",
    "            source_dir=source_dir,\n",
    "            target_dir=target_dir,\n",
    "            debug=False,\n",
    "            train_ratio=train_ratio,\n",
    "            glove_corpus=\"6B\",\n",
    "            glove_dir=glove_dir,\n",
    "            glove_vec_size=glove_vec_size,\n",
    "            mode=mode,\n",
    "            single_path=\"\",\n",
    "            tokenizer=tokenizer,\n",
    "            url=url,\n",
    "            port=port,\n",
    "            split=False,\n",
    "        )\n",
    "        print(args)\n",
    "        return args\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train SQUAD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    prepro_dir: InputPath(str),\n",
    "    sent_size_th,\n",
    "    ques_size_th,\n",
    "    num_epochs,\n",
    "    num_steps,\n",
    "    eval_period,\n",
    "    save_period,\n",
    "    learning_rate,\n",
    "    batch_size,\n",
    "    hidden_size,\n",
    "    var_decay,\n",
    "    model_dir: OutputPath(str),\n",
    "):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    from basic.cli import main\n",
    "\n",
    "    model_name = \"basic\"\n",
    "    data_dir = prepro_dir + \"/squad\"\n",
    "    output_dir = model_dir + \"/out/squad\"\n",
    "    argv = [\n",
    "        \"./basic/cli.py\",\n",
    "        \"--model_name\",\n",
    "        model_name,\n",
    "        \"--data_dir\",\n",
    "        data_dir,\n",
    "        \"--out_base_dir\",\n",
    "        output_dir,\n",
    "        \"--noload\",\n",
    "        \"--dev_name\",\n",
    "        \"dev\",\n",
    "        \"--sent_size_th\",\n",
    "        sent_size_th,\n",
    "        \"--ques_size_th\",\n",
    "        ques_size_th,\n",
    "        \"--num_epochs\",\n",
    "        num_epochs,\n",
    "        \"--num_steps\",\n",
    "        num_steps,\n",
    "        \"--eval_period\",\n",
    "        eval_period,\n",
    "        \"--save_period\",\n",
    "        save_period,\n",
    "        \"--init_lr\",\n",
    "        learning_rate,\n",
    "        \"--batch_size\",\n",
    "        batch_size,\n",
    "        \"--hidden_size\",\n",
    "        hidden_size,\n",
    "        \"--var_decay\",\n",
    "        var_decay,\n",
    "    ]\n",
    "    tf.app.run(main, argv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test SQUAD model and generate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    prepro_dir: InputPath(str),\n",
    "    prev_model_dir: InputPath(str),\n",
    "    sent_size_th,\n",
    "    ques_size_th,\n",
    "    num_epochs,\n",
    "    num_steps,\n",
    "    eval_period,\n",
    "    save_period,\n",
    "    learning_rate,\n",
    "    batch_size,\n",
    "    hidden_size,\n",
    "    var_decay,\n",
    "    mlpipeline_metrics_path: OutputPath(\"Metrics\"),\n",
    "    model_dir: OutputPath(str),\n",
    "):\n",
    "\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    src = prev_model_dir + \"/out/squad\"\n",
    "    dst = model_dir + \"/out/squad\"\n",
    "    shutil.copytree(src, dst)\n",
    "\n",
    "    model_name = \"basic\"\n",
    "    data_dir = prepro_dir + \"/squad\"\n",
    "    output_dir = model_dir + \"/out/squad\"\n",
    "\n",
    "    flags = tf.app.flags\n",
    "\n",
    "    # Names and directories\n",
    "    flags.DEFINE_string(\"model_name\", model_name, \"Model name [basic | basic-class]\")\n",
    "    flags.DEFINE_string(\"data_dir\", data_dir, \"Data dir [data/squad]\")\n",
    "    flags.DEFINE_string(\"run_id\", \"0\", \"Run ID [0]\")\n",
    "    flags.DEFINE_string(\"out_base_dir\", output_dir, \"out base dir [out]\")\n",
    "    flags.DEFINE_string(\"forward_name\", \"single\", \"Forward name [single]\")\n",
    "    flags.DEFINE_string(\"answer_path\", \"\", \"Answer path []\")\n",
    "    flags.DEFINE_string(\"eval_path\", \"\", \"Eval path []\")\n",
    "    flags.DEFINE_string(\"load_path\", \"\", \"Load path []\")\n",
    "    flags.DEFINE_string(\"shared_path\", \"\", \"Shared path []\")\n",
    "\n",
    "    # Device placement\n",
    "    flags.DEFINE_string(\n",
    "        \"device\", \"/cpu:0\", \"default device for summing gradients. [/cpu:0]\"\n",
    "    )\n",
    "    flags.DEFINE_string(\n",
    "        \"device_type\",\n",
    "        \"gpu\",\n",
    "        \"device for computing gradients (parallelization). cpu | gpu [gpu]\",\n",
    "    )\n",
    "    flags.DEFINE_integer(\n",
    "        \"num_gpus\", 1, \"num of gpus or cpus for computing gradients [1]\"\n",
    "    )\n",
    "\n",
    "    # Essential training and test options\n",
    "    flags.DEFINE_string(\"mode\", \"test\", \"train | test | forward [test]\")\n",
    "    flags.DEFINE_boolean(\"load\", True, \"load saved data? [True]\")\n",
    "    flags.DEFINE_bool(\"single\", False, \"supervise only the answer sentence? [False]\")\n",
    "    flags.DEFINE_boolean(\"debug\", False, \"Debugging mode? [False]\")\n",
    "    flags.DEFINE_bool(\n",
    "        \"load_ema\", True, \"load exponential average of variables when testing?  [True]\"\n",
    "    )\n",
    "    flags.DEFINE_bool(\"eval\", True, \"eval? [True]\")\n",
    "    flags.DEFINE_bool(\"train_only_output\", False, \"Train only output module?\")\n",
    "    flags.DEFINE_bool(\"load_trained_model\", False, \"Load SQUAD trained model\")\n",
    "    flags.DEFINE_bool(\"freeze_phrase_layer\", False, \"Freeze phrase layer\")\n",
    "    flags.DEFINE_bool(\"freeze_att_layer\", False, \"Freeze att layer\")\n",
    "    flags.DEFINE_bool(\n",
    "        \"freeze_span_modelling_layer\", False, \"Freeze modelling layer for span\"\n",
    "    )\n",
    "\n",
    "    flags.DEFINE_bool(\"using_shared\", False, \"using pre-created shared.json\")\n",
    "    flags.DEFINE_bool(\"load_shared\", False, \"load shared.json for each batch\")\n",
    "    flags.DEFINE_string(\"dev_name\", \"test\", \"using dev or test?\")\n",
    "    flags.DEFINE_string(\"test_name\", \"dev\", \"using test or dev?\")\n",
    "\n",
    "    # Training / test parameters\n",
    "    flags.DEFINE_integer(\"batch_size\", int(batch_size), \"Batch size [60]\")\n",
    "    flags.DEFINE_integer(\"val_num_batches\", 100, \"validation num batches [100]\")\n",
    "    flags.DEFINE_integer(\"test_num_batches\", 0, \"test num batches [0]\")\n",
    "    flags.DEFINE_integer(\n",
    "        \"num_epochs\", int(num_epochs), \"Total number of epochs for training [12]\"\n",
    "    )\n",
    "    flags.DEFINE_integer(\"num_steps\", int(num_steps), \"Number of steps [20000]\")\n",
    "    flags.DEFINE_integer(\"load_step\", 0, \"load step [0]\")\n",
    "    flags.DEFINE_float(\"init_lr\", float(learning_rate), \"Initial learning rate [0.5]\")\n",
    "    flags.DEFINE_float(\n",
    "        \"input_keep_prob\", 0.8, \"Input keep prob for the dropout of LSTM weights [0.8]\"\n",
    "    )\n",
    "    flags.DEFINE_float(\n",
    "        \"keep_prob\", 0.8, \"Keep prob for the dropout of Char-CNN weights [0.8]\"\n",
    "    )\n",
    "    flags.DEFINE_float(\"wd\", 0.0, \"L2 weight decay for regularization [0.0]\")\n",
    "    flags.DEFINE_integer(\"hidden_size\", int(hidden_size), \"Hidden size [100]\")\n",
    "    flags.DEFINE_integer(\"char_out_size\", 100, \"char-level word embedding size [100]\")\n",
    "    flags.DEFINE_integer(\"char_emb_size\", 8, \"Char emb size [8]\")\n",
    "    flags.DEFINE_string(\n",
    "        \"out_channel_dims\",\n",
    "        \"100\",\n",
    "        \"Out channel dims of Char-CNN, separated by commas [100]\",\n",
    "    )\n",
    "    flags.DEFINE_string(\n",
    "        \"filter_heights\", \"5\", \"Filter heights of Char-CNN, separated by commas [5]\"\n",
    "    )\n",
    "    flags.DEFINE_bool(\"finetune\", False, \"Finetune word embeddings? [False]\")\n",
    "    flags.DEFINE_bool(\"highway\", True, \"Use highway? [True]\")\n",
    "    flags.DEFINE_integer(\"highway_num_layers\", 2, \"highway num layers [2]\")\n",
    "    flags.DEFINE_bool(\"share_cnn_weights\", True, \"Share Char-CNN weights [True]\")\n",
    "    flags.DEFINE_bool(\n",
    "        \"share_lstm_weights\",\n",
    "        True,\n",
    "        \"Share pre-processing (phrase-level) LSTM weights [True]\",\n",
    "    )\n",
    "    flags.DEFINE_float(\n",
    "        \"var_decay\",\n",
    "        float(var_decay),\n",
    "        \"Exponential moving average decay for variables [0.999]\",\n",
    "    )\n",
    "    flags.DEFINE_string(\"classifier\", \"maxpool\", \"[maxpool, sumpool, default]\")\n",
    "\n",
    "    # Optimizations\n",
    "    flags.DEFINE_bool(\"cluster\", True, \"Cluster data for faster training [False]\")\n",
    "    flags.DEFINE_bool(\"len_opt\", True, \"Length optimization? [False]\")\n",
    "    flags.DEFINE_bool(\n",
    "        \"cpu_opt\", False, \"CPU optimization? GPU computation can be slower [False]\"\n",
    "    )\n",
    "\n",
    "    # Logging and saving options\n",
    "    flags.DEFINE_boolean(\"progress\", True, \"Show progress? [True]\")\n",
    "    flags.DEFINE_integer(\"log_period\", 100, \"Log period [100]\")\n",
    "    flags.DEFINE_integer(\"eval_period\", int(eval_period), \"Eval period [1000]\")\n",
    "    flags.DEFINE_integer(\"save_period\", int(save_period), \"Save Period [1000]\")\n",
    "    flags.DEFINE_integer(\"max_to_keep\", 20, \"Max recent saves to keep [20]\")\n",
    "    flags.DEFINE_bool(\"dump_eval\", True, \"dump eval? [True]\")\n",
    "    flags.DEFINE_bool(\"dump_answer\", False, \"dump answer? [True]\")\n",
    "    flags.DEFINE_bool(\"vis\", False, \"output visualization numbers? [False]\")\n",
    "    flags.DEFINE_bool(\"dump_pickle\", True, \"Dump pickle instead of json? [True]\")\n",
    "    flags.DEFINE_float(\n",
    "        \"decay\", 0.9, \"Exponential moving average decay for logging values [0.9]\"\n",
    "    )\n",
    "\n",
    "    # Thresholds for speed and less memory usage\n",
    "    flags.DEFINE_integer(\"word_count_th\", 10, \"word count th [100]\")\n",
    "    flags.DEFINE_integer(\"char_count_th\", 50, \"char count th [500]\")\n",
    "    flags.DEFINE_integer(\"sent_size_th\", int(sent_size_th), \"sent size th [64]\")\n",
    "    flags.DEFINE_integer(\"num_sents_th\", 1, \"num sents th [8]\")\n",
    "    flags.DEFINE_integer(\"ques_size_th\", int(ques_size_th), \"ques size th [32]\")\n",
    "    flags.DEFINE_integer(\"word_size_th\", 16, \"word size th [16]\")\n",
    "    flags.DEFINE_integer(\"para_size_th\", 256, \"para size th [256]\")\n",
    "\n",
    "    # Advanced training options\n",
    "    flags.DEFINE_bool(\"lower_word\", True, \"lower word [True]\")\n",
    "    flags.DEFINE_bool(\"squash\", False, \"squash the sentences into one? [False]\")\n",
    "    flags.DEFINE_bool(\"swap_memory\", True, \"swap memory? [True]\")\n",
    "    flags.DEFINE_string(\"data_filter\", \"max\", \"max | valid | semi [max]\")\n",
    "    flags.DEFINE_bool(\"use_glove_for_unk\", True, \"use glove for unk [False]\")\n",
    "    flags.DEFINE_bool(\n",
    "        \"known_if_glove\", True, \"consider as known if present in glove [False]\"\n",
    "    )\n",
    "    flags.DEFINE_string(\"logit_func\", \"tri_linear\", \"logit func [tri_linear]\")\n",
    "    flags.DEFINE_string(\"answer_func\", \"linear\", \"answer logit func [linear]\")\n",
    "    flags.DEFINE_string(\"sh_logit_func\", \"tri_linear\", \"sh logit func [tri_linear]\")\n",
    "\n",
    "    # Ablation options\n",
    "    flags.DEFINE_bool(\"use_char_emb\", True, \"use char emb? [True]\")\n",
    "    flags.DEFINE_bool(\"use_word_emb\", True, \"use word embedding? [True]\")\n",
    "    flags.DEFINE_bool(\"q2c_att\", True, \"question-to-context attention? [True]\")\n",
    "    flags.DEFINE_bool(\"c2q_att\", True, \"context-to-question attention? [True]\")\n",
    "    flags.DEFINE_bool(\"dynamic_att\", False, \"Dynamic attention [False]\")\n",
    "\n",
    "    def main(_):\n",
    "        from basic.main import main as m\n",
    "\n",
    "        config = flags.FLAGS\n",
    "        config.out_dir = os.path.join(\n",
    "            config.out_base_dir, config.model_name, str(config.run_id).zfill(2)\n",
    "        )\n",
    "        evaluator = m(config)\n",
    "\n",
    "        \"\"\"Generating metrics for the squad model\"\"\"\n",
    "        metrics = {\n",
    "            \"metrics\": [\n",
    "                {\n",
    "                    \"name\": \"accuracy-score\",\n",
    "                    \"numberValue\": str(evaluator.acc),\n",
    "                    \"format\": \"RAW\",\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"f1-score\",\n",
    "                    \"numberValue\": str(evaluator.f1),\n",
    "                    \"format\": \"RAW\",\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "        import json\n",
    "\n",
    "        with open(mlpipeline_metrics_path, \"w\") as f:\n",
    "            json.dump(metrics, f)\n",
    "\n",
    "    tf.app.run(main)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transform python functions into components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_op = func_to_container_op(\n",
    "        download,\n",
    "        base_image=\"tensorflow/tensorflow:latest-gpu-py3\",\n",
    "        packages_to_install=[\"tqdm\"],\n",
    "    )\n",
    "squad_preprocess_op = func_to_container_op(\n",
    "        prepro_basic,\n",
    "        base_image=\"sciling/tensorflow:0.12.0-py3\",\n",
    "        packages_to_install=[\n",
    "            \"https://github.com/sciling/qatransfer/archive/refs/heads/master.zip#egg=qatransfer\",\n",
    "        ]\n",
    "    )\n",
    "squad_span_pretrain_op = func_to_container_op(\n",
    "        train,\n",
    "        base_image=\"sciling/tensorflow:0.12.0-py3\",\n",
    "        packages_to_install=[\n",
    "            \"https://github.com/sciling/qatransfer/archive/refs/heads/master.zip#egg=qatransfer\",\n",
    "        ]\n",
    "    )\n",
    "squad_test_op = func_to_container_op(\n",
    "        test,\n",
    "        base_image=\"sciling/tensorflow:0.12.0-py3\",\n",
    "        packages_to_install=[\n",
    "            \"https://github.com/sciling/qatransfer/archive/refs/heads/master.zip#egg=qatransfer\"\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define pipeline and component connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(name=\"pipeline_model_squad\", description=\"\")\n",
    "def qa_pipeline(\n",
    "    prepro_train_ratio: float = 0.9,\n",
    "    prepro_glove_vec_size: int = 100,\n",
    "    prepro_mode: str = \"full\",\n",
    "    prepro_tokenizer: str = \"PTB\",\n",
    "    prepro_url: str = \"vision-server2.corp.ai2\",\n",
    "    prepro_port: int = 8000,\n",
    "    train_sent_size_th: str = \"500\",\n",
    "    train_ques_size_th: str = \"30\",\n",
    "    train_num_epochs: str = \"12\",\n",
    "    train_num_steps: str = \"20000\",\n",
    "    train_eval_period: str = \"500\",\n",
    "    train_save_period: str = \"500\",\n",
    "    train_learning_rate: float = 0.5,\n",
    "    train_batch_size: int = 60,\n",
    "    train_hidden_size: int = 100,\n",
    "    train_var_decay: float = 0.999,\n",
    "):\n",
    "    dataset_path = download_op()\n",
    "    prepro_span = squad_preprocess_op(\n",
    "        dataset_path.output,\n",
    "        prepro_train_ratio,\n",
    "        prepro_glove_vec_size,\n",
    "        prepro_mode,\n",
    "        prepro_tokenizer,\n",
    "        prepro_url,\n",
    "        prepro_port,\n",
    "    )\n",
    "    model = squad_span_pretrain_op(\n",
    "        prepro_span.output,\n",
    "        train_sent_size_th,\n",
    "        train_ques_size_th,\n",
    "        train_num_epochs,\n",
    "        train_num_steps,\n",
    "        train_eval_period,\n",
    "        train_save_period,\n",
    "        train_learning_rate,\n",
    "        train_batch_size,\n",
    "        train_hidden_size,\n",
    "        train_var_decay,\n",
    "    ).set_memory_request(\"4G\")\n",
    "    squad_test_op(\n",
    "        prepro_span.output,\n",
    "        model.output,\n",
    "        train_sent_size_th,\n",
    "        train_ques_size_th,\n",
    "        train_num_epochs,\n",
    "        train_num_steps,\n",
    "        train_eval_period,\n",
    "        train_save_period,\n",
    "        train_learning_rate,\n",
    "        train_batch_size,\n",
    "        train_hidden_size,\n",
    "        train_var_decay,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Connect with AWS Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "# Disable ssl verification\n",
    "from kfp_server_api.configuration import Configuration as Configuration\n",
    "if 'old_init' not in globals():\n",
    "    old_init = Configuration.__init__\n",
    "def new_init(self, *k, **kw):\n",
    "    old_init(self, *k, **kw)\n",
    "    self.verify_ssl = False\n",
    "Configuration.__init__ = new_init\n",
    "cookies = \"authservice_session=YOUR_TOKEN\"\n",
    "client = kfp.Client(host='http://istio-ingressgateway.istio-system.svc/pipeline', namespace='admin', cookies=cookies)\n",
    "client.list_experiments(namespace=\"admin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = qa_pipeline\n",
    "experiment_name = 'squad_experiment'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  '{}.zip'.format(experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Define arguments and create a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGUMENTS DEFINITION\n",
    "arguments = {\n",
    "    'train_sent_size_th' : '10',\n",
    "    'train_ques_size_th':'10', \n",
    "    \"train_num_epochs\" : '1',\n",
    "    \"train_num_steps\" : '1',\n",
    "    \"train_eval_period\" : '1',\n",
    "    \"train_save_period\" : '1',\n",
    "}\n",
    "\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  namespace='admin',\n",
    "                                                  arguments=arguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
