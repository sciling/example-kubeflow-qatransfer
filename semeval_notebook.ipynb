{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semeval Question Answering through Transfer Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install necessary packages and restart de kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip requirements\n",
    "!python -m pip install --user --upgrade pip\n",
    "!pip install --user --upgrade kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow SDK\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import OutputPath, InputPath, func_to_container_op, OutputBinaryFile, InputBinaryFile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(squad_url, dataset_path: OutputPath()):\n",
    "    import os\n",
    "    import tempfile\n",
    "    import zipfile\n",
    "\n",
    "    import requests\n",
    "\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Download SemEval\n",
    "    SEMEVAL = dataset_path + \"/semeval\"\n",
    "    os.makedirs(SEMEVAL, exist_ok=True)\n",
    "    r_semeval = requests.get(\n",
    "        \"http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-cqa-ql-traindev-v3.2.zip\"\n",
    "    )\n",
    "    total_size_in_bytes = int(r_semeval.headers.get(\"content-length\", 0))\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n",
    "    with tempfile.TemporaryFile() as tf:\n",
    "        for chunk in r_semeval.iter_content(chunk_size=1024):\n",
    "            progress_bar.update(len(chunk))\n",
    "            tf.write(chunk)\n",
    "        with zipfile.ZipFile(tf, \"r\") as f:\n",
    "            f.extractall(SEMEVAL)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "\n",
    "    r_semeval_test = requests.get(\n",
    "        \"http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016_task3_test.zip\"\n",
    "    )\n",
    "    total_size_in_bytes = int(r_semeval_test.headers.get(\"content-length\", 0))\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n",
    "    with tempfile.TemporaryFile() as tf:\n",
    "        for chunk in r_semeval_test.iter_content(chunk_size=1024):\n",
    "            progress_bar.update(len(chunk))\n",
    "            tf.write(chunk)\n",
    "        with zipfile.ZipFile(tf, \"r\") as f:\n",
    "            f.extractall(SEMEVAL)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "\n",
    "    # Download GloVe\n",
    "    GLOVE_DIR = dataset_path + \"/glove\"\n",
    "    os.makedirs(GLOVE_DIR, exist_ok=True)\n",
    "    r = requests.get(\"http://nlp.stanford.edu/data/glove.6B.zip\", stream=True)\n",
    "    total_size_in_bytes = int(r.headers.get(\"content-length\", 0))\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n",
    "    with tempfile.TemporaryFile() as tf:\n",
    "        for chunk in r.iter_content(chunk_size=1024):\n",
    "            progress_bar.update(len(chunk))\n",
    "            tf.write(chunk)\n",
    "        with zipfile.ZipFile(tf, \"r\") as f:\n",
    "            f.extractall(GLOVE_DIR)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "\n",
    "    r_squad = requests.get(squad_url)\n",
    "    total_size_in_bytes = int(r_squad.headers.get(\"content-length\", 0))\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n",
    "    with tempfile.TemporaryFile() as tf:\n",
    "        for chunk in r_squad.iter_content(chunk_size=1024):\n",
    "            progress_bar.update(len(chunk))\n",
    "            tf.write(chunk)\n",
    "        with zipfile.ZipFile(tf, \"r\") as f:\n",
    "            f.extractall(dataset_path)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "\n",
    "    print(os.listdir(dataset_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess semeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semeval_prepro(dataset_path: InputPath(str), semeval_path: OutputPath(str)):\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    import nltk\n",
    "    import numpy as np\n",
    "    import xmltodict\n",
    "\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "    def prepro(args):\n",
    "        if not os.path.exists(args.target_dir):\n",
    "            os.makedirs(args.target_dir)\n",
    "        prepro_each(args, \"train\")\n",
    "        prepro_each(args, \"dev\")\n",
    "        prepro_each(args, \"test\")\n",
    "\n",
    "    def get_args():\n",
    "        source_dir = dataset_path + \"/semeval\"\n",
    "        target_dir = semeval_path + \"/semeval\"\n",
    "        glove_dir = dataset_path + \"/glove\"\n",
    "\n",
    "        # Create args object\n",
    "        from types import SimpleNamespace\n",
    "\n",
    "        args = SimpleNamespace(\n",
    "            source_dir=source_dir, target_dir=target_dir, glove_dir=glove_dir\n",
    "        )\n",
    "        return args\n",
    "\n",
    "    def save(target_dir, data, shared, data_type):\n",
    "        data_path = os.path.join(target_dir, \"data_{}.json\".format(data_type))\n",
    "        shared_path = os.path.join(target_dir, \"shared_{}.json\".format(data_type))\n",
    "        json.dump(data, open(data_path, \"w\"))\n",
    "        json.dump(shared, open(shared_path, \"w\"))\n",
    "\n",
    "    def get_word2vec(glove_dir, word_counter):\n",
    "        glove_corpus = \"6B\"\n",
    "        glove_vec_size = 100\n",
    "        glove_path = os.path.join(\n",
    "            glove_dir, \"glove.{}.{}d.txt\".format(glove_corpus, glove_vec_size)\n",
    "        )\n",
    "        sizes = {\n",
    "            \"6B\": int(4e5),\n",
    "            \"42B\": int(1.9e6),\n",
    "            \"840B\": int(2.2e6),\n",
    "            \"2B\": int(1.2e6),\n",
    "        }\n",
    "        total = sizes[glove_corpus]\n",
    "        word2vec_dict = {}\n",
    "        with open(glove_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "            for line in tqdm(fh, total=total):\n",
    "                array = line.lstrip().rstrip().split(\" \")\n",
    "                word = array[0]\n",
    "                vector = list(map(float, array[1:]))\n",
    "                if word in word_counter:\n",
    "                    word2vec_dict[word] = vector\n",
    "                elif word.capitalize() in word_counter:\n",
    "                    word2vec_dict[word.capitalize()] = vector\n",
    "                elif word.lower() in word_counter:\n",
    "                    word2vec_dict[word.lower()] = vector\n",
    "                elif word.upper() in word_counter:\n",
    "                    word2vec_dict[word.upper()] = vector\n",
    "\n",
    "        print(\n",
    "            \"{}/{} of word vocab have corresponding vectors in {}\".format(\n",
    "                len(word2vec_dict), len(word_counter), glove_path\n",
    "            )\n",
    "        )\n",
    "        return word2vec_dict\n",
    "\n",
    "    def word_tokenize(tokens):\n",
    "        return [\n",
    "            token.replace(\"''\", '\"').replace(\"``\", '\"')\n",
    "            for token in nltk.word_tokenize(tokens)\n",
    "        ]\n",
    "\n",
    "    def prepro_each(args, data_type):\n",
    "\n",
    "        data_list = []\n",
    "        sub_dir = (\n",
    "            \"SemEval2016_task3_test/English\"\n",
    "            if data_type == \"test\"\n",
    "            else \"v3.2/%s\" % (data_type)\n",
    "        )\n",
    "        fileName = \"SemEval2016-Task3-CQA-QL-%s-subtaskA.xml\"\n",
    "        if data_type == \"train\":\n",
    "            with open(\n",
    "                os.path.join(args.source_dir, sub_dir, fileName % (\"train-part1\")),\n",
    "                encoding=\"utf-8\",\n",
    "            ) as f:\n",
    "                data_list += xmltodict.parse(f.read())[\"xml\"][\"Thread\"]\n",
    "            with open(\n",
    "                os.path.join(args.source_dir, sub_dir, fileName % (\"train-part2\")),\n",
    "                encoding=\"utf-8\",\n",
    "            ) as f:\n",
    "                data_list += xmltodict.parse(f.read())[\"xml\"][\"Thread\"]\n",
    "        else:\n",
    "            with open(\n",
    "                os.path.join(args.source_dir, sub_dir, fileName % (data_type)),\n",
    "                encoding=\"utf-8\",\n",
    "            ) as f:\n",
    "                data_list += xmltodict.parse(f.read())[\"xml\"][\"Thread\"]\n",
    "        questions, comments, answers, question_ids, answer_ids = [], [], [], [], []\n",
    "\n",
    "        text2answer = {\"Good\": 0, \"PotentiallyUseful\": 1, \"Bad\": 2}\n",
    "        for data in data_list:\n",
    "            q = str(data[\"RelQuestion\"][\"RelQBody\"])\n",
    "            qid = data[\"RelQuestion\"][\"@RELQ_ID\"]\n",
    "            cs = data[\"RelComment\"]\n",
    "            comment = [c[\"RelCText\"] for c in cs]\n",
    "            answer = [text2answer[c[\"@RELC_RELEVANCE2RELQ\"]] for c in cs]\n",
    "            cids = [c[\"@RELC_ID\"] for c in cs]\n",
    "            questions.append(q)\n",
    "            comments.append(comment)\n",
    "            answers.append(answer)\n",
    "            question_ids.append(qid)\n",
    "            answer_ids.append(cids)\n",
    "\n",
    "        q, cq, y, rx, rcx, ids, idxs = [], [], [], [], [], [], []\n",
    "        char_counter, lower_word_counter = Counter(), Counter()\n",
    "\n",
    "        print(\"start for preprocessing for %s\" % (data_type))\n",
    "\n",
    "        q_len, x_len = [], []\n",
    "        for ai, (question, comment, answer, question_id, answer_id) in tqdm(\n",
    "            enumerate(zip(questions, comments, answers, question_ids, answer_ids))\n",
    "        ):\n",
    "            qi = word_tokenize(question)\n",
    "            q_len.append(len(qi))\n",
    "            cqi = [list(qij) for qij in qi]\n",
    "            for qij in qi:\n",
    "                lower_word_counter[qij.lower()] += 1\n",
    "                for qijk in qij:\n",
    "                    char_counter[qijk] += 1\n",
    "            for pi, (story, yi, a_id) in enumerate(zip(comment, answer, answer_id)):\n",
    "                rxi = [ai, pi]\n",
    "                xi = [word_tokenize(story)]\n",
    "                x_len.append(len(xi[0]))\n",
    "                cxi = [list(xij) for xij in xi]\n",
    "                for xij in xi:\n",
    "                    for xijk in xij:\n",
    "                        lower_word_counter[xijk.lower()] += 1\n",
    "                        for xijkl in xijk:\n",
    "                            char_counter[xijkl] += 1\n",
    "                json.dump(\n",
    "                    {\"x\": xi, \"cx\": cxi, \"p\": story},\n",
    "                    open(\n",
    "                        os.path.join(\n",
    "                            args.target_dir,\n",
    "                            \"shared_%s_%s_%s.json\"\n",
    "                            % (data_type, str(ai).zfill(3), str(pi).zfill(3)),\n",
    "                        ),\n",
    "                        \"w\",\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                def put():\n",
    "                    q.append(qi)\n",
    "                    cq.append(cqi)\n",
    "                    y.append(yi)\n",
    "                    rx.append(rxi)\n",
    "                    rcx.append(rxi)\n",
    "                    ids.append((question_id, a_id))\n",
    "                    idxs.append(len(idxs))\n",
    "\n",
    "                put()\n",
    "                if data_type == \"train\" and yi == 0:\n",
    "                    for t in range(3):\n",
    "                        put()\n",
    "                    if np.random.randint(10) > 0:\n",
    "                        put()\n",
    "                elif data_type == \"train\" and yi == 2:\n",
    "                    for t in range(3):\n",
    "                        put()\n",
    "        lower_word2vec_dict = get_word2vec(args.glove_dir, lower_word_counter)\n",
    "        data = {\n",
    "            \"q\": q,\n",
    "            \"cq\": cq,\n",
    "            \"y\": y,\n",
    "            \"*x\": rx,\n",
    "            \"*cx\": rcx,\n",
    "            \"idxs\": idxs,\n",
    "            \"ids\": ids,\n",
    "            \"*p\": rx,\n",
    "        }\n",
    "        shared = {\n",
    "            \"char_counter\": char_counter,\n",
    "            \"lower_word_counter\": lower_word_counter,\n",
    "            \"lower_word2vec\": lower_word2vec_dict,\n",
    "        }\n",
    "        print(\"saving ...\")\n",
    "        save(args.target_dir, data, shared, data_type)\n",
    "\n",
    "    args = get_args()\n",
    "    prepro(args)\n",
    "\n",
    "    print(os.listdir(semeval_path + \"/semeval\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Semeval train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semeval_train(\n",
    "    dataset_path: InputPath(str),\n",
    "    semeval_path: InputPath(str),\n",
    "    load_path,\n",
    "    shared_path,\n",
    "    run_id,\n",
    "    sent_size_th,\n",
    "    ques_size_th,\n",
    "    num_epochs,\n",
    "    num_steps,\n",
    "    eval_period,\n",
    "    save_period,\n",
    "    model_path: OutputPath(str),\n",
    "):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    from basic.cli import main\n",
    "\n",
    "    input_dir = semeval_path + \"/semeval\"\n",
    "    output_dir = model_path + \"/out/semeval\"\n",
    "    full_load_path = dataset_path + load_path\n",
    "    full_shared_path = dataset_path + shared_path\n",
    "    tf.app.run(\n",
    "        main,\n",
    "        argv=[\n",
    "            \"./basic/cli.py\",\n",
    "            \"--data_dir\",\n",
    "            input_dir,\n",
    "            \"--out_base_dir\",\n",
    "            output_dir,\n",
    "            \"--load_path\",\n",
    "            full_load_path,\n",
    "            \"--shared_path\",\n",
    "            full_shared_path,\n",
    "            \"--load_trained_model\",\n",
    "            \"--run_id\",\n",
    "            run_id,\n",
    "            \"--load_shared\",\n",
    "            \"--nocluster\",\n",
    "            \"--sent_size_th\",\n",
    "            sent_size_th,\n",
    "            \"--ques_size_th\",\n",
    "            ques_size_th,\n",
    "            \"--num_epochs\",\n",
    "            num_epochs,\n",
    "            \"--num_steps\",\n",
    "            num_steps,\n",
    "            \"--eval_period\",\n",
    "            eval_period,\n",
    "            \"--save_period\",\n",
    "            save_period,\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semeval test file generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_semeval_test_files(\n",
    "    semeval_path: InputPath(str),\n",
    "    model_path: InputPath(str),\n",
    "    start_step,\n",
    "    end_step,\n",
    "    eval_period,\n",
    "    run_ids,\n",
    "    threshold,\n",
    "    test_path: OutputPath(str),\n",
    "):\n",
    "    import os\n",
    "\n",
    "    from semeval.result import evaluate\n",
    "    from semeval.result import load_data\n",
    "\n",
    "    end_step = int(end_step)\n",
    "    start_step = int(start_step)\n",
    "    eval_period = int(eval_period)\n",
    "    threshold = float(threshold)\n",
    "\n",
    "    def get_args():\n",
    "        data_dir = semeval_path + \"/semeval\"\n",
    "        eval_dir = model_path + \"/out/semeval/basic-class\"\n",
    "        store_dir = test_path + \"/semeval/store\"\n",
    "        from types import SimpleNamespace\n",
    "\n",
    "        args = SimpleNamespace(\n",
    "            data_dir=data_dir,\n",
    "            end_step=end_step,\n",
    "            ensemble=False,\n",
    "            eval_dir=eval_dir,\n",
    "            eval_name=\"test\",\n",
    "            eval_period=eval_period,\n",
    "            run_ids=run_ids,\n",
    "            start_step=start_step,\n",
    "            steps=\"\",\n",
    "            store_dir=store_dir,\n",
    "            threshold=threshold,\n",
    "        )\n",
    "        return args\n",
    "\n",
    "    def main():\n",
    "        args = get_args()\n",
    "        print(args)\n",
    "        if args.ensemble:\n",
    "            print(\"Ensemble not implemented yet\")\n",
    "            return\n",
    "\n",
    "        data = load_data(args)\n",
    "        if not os.path.exists(args.store_dir):\n",
    "            os.makedirs(args.store_dir)\n",
    "\n",
    "        for i, run_id in enumerate(args.run_ids.split(\",\")):\n",
    "            for step in range(args.start_step, args.end_step, args.eval_period):\n",
    "                evaluate(\n",
    "                    args,\n",
    "                    data,\n",
    "                    run_id,\n",
    "                    step,\n",
    "                    dump_gold=(i == 0 and step == args.start_step),\n",
    "                )\n",
    "\n",
    "    main()\n",
    "    print(\"Generated files:\")\n",
    "    print(os.listdir(test_path + \"/semeval/store\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semeval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semeval_test(\n",
    "    test_path: InputPath(str),\n",
    "    run_id,\n",
    "    start_step,\n",
    "    end_step,\n",
    "    th,\n",
    "    reranking_th,\n",
    "    op_format,\n",
    "    verbose,\n",
    "    ignore_noanswer,\n",
    "):\n",
    "    import sys\n",
    "\n",
    "    from semeval.evaluation.MAP_scripts.ev import eval_reranker\n",
    "    from semeval.evaluation.MAP_scripts.ev import eval_search_engine\n",
    "\n",
    "    th = int(th)\n",
    "    reranking_th = int(reranking_th)\n",
    "\n",
    "    def main(options, args):\n",
    "        if len(args) == 1:\n",
    "            res_fname = args[0]\n",
    "            eval_search_engine(res_fname, options[\"format\"], options[\"th\"])\n",
    "        elif len(args) == 2:\n",
    "            res_fname = args[0]\n",
    "            pred_fname = args[1]\n",
    "            eval_reranker(\n",
    "                res_fname,\n",
    "                pred_fname,\n",
    "                options[\"format\"],\n",
    "                options[\"th\"],\n",
    "                options[\"verbose\"],\n",
    "                options[\"reranking_th\"],\n",
    "                options[\"ignore_noanswer\"],\n",
    "            )\n",
    "        else:\n",
    "            sys.exit(1)\n",
    "\n",
    "    start_step = int(start_step)\n",
    "    end_step = int(end_step)\n",
    "    if verbose == \"False\":\n",
    "        verbose = False\n",
    "    else:\n",
    "        verbose = True\n",
    "    if ignore_noanswer == \"False\":\n",
    "        ignore_noanswer = False\n",
    "    else:\n",
    "        ignore_noanswer = True\n",
    "    options = {\n",
    "        \"th\": th,\n",
    "        \"reranking_th\": reranking_th,\n",
    "        \"format\": op_format,\n",
    "        \"verbose\": verbose,\n",
    "        \"ignore_noanswer\": ignore_noanswer,\n",
    "    }\n",
    "    for step in range(start_step, end_step + 1, 200):\n",
    "        args = [\n",
    "            test_path + \"/semeval/store/test-gold\",\n",
    "            test_path + \"/semeval/store/test-\" + run_id + \"-\" + str(step).zfill(6),\n",
    "        ]\n",
    "        main(options, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transform python functions into components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_op = func_to_container_op(\n",
    "        download,\n",
    "        base_image=\"tensorflow/tensorflow:latest-gpu-py3\",\n",
    "        packages_to_install=[\"tqdm\"],\n",
    "    )\n",
    "semeval_prepro_op = func_to_container_op(\n",
    "        semeval_prepro,\n",
    "        base_image=\"sciling/tensorflow:0.12.0-py3\",\n",
    "        packages_to_install=[\n",
    "            \"https://github.com/sciling/qatransfer/archive/refs/heads/master.zip#egg=qatransfer\"\n",
    "        ],\n",
    "    )\n",
    "semeval_train_op = func_to_container_op(\n",
    "        semeval_train,\n",
    "        base_image=\"sciling/tensorflow:0.12.0-py3\",\n",
    "        packages_to_install=[\n",
    "            \"https://github.com/sciling/qatransfer/archive/refs/heads/master.zip#egg=qatransfer\"\n",
    "        ],\n",
    "    )\n",
    "semeval_generate_test_files_op = func_to_container_op(\n",
    "        generate_semeval_test_files,\n",
    "        base_image=\"sciling/tensorflow:0.12.0-py3\",\n",
    "        packages_to_install=[\n",
    "            \"https://github.com/sciling/qatransfer/archive/refs/heads/master.zip#egg=qatransfer\"\n",
    "        ],\n",
    "    )\n",
    "semeval_test_op = func_to_container_op(\n",
    "        semeval_test,\n",
    "        base_image=\"sciling/tensorflow:0.12.0-py3\",\n",
    "        packages_to_install=[\n",
    "            \"https://github.com/sciling/qatransfer/archive/refs/heads/master.zip#egg=qatransfer\"\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define pipeline and component connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@dsl.pipeline(name=\"question-answering-pipeline\", description=\"\")\n",
    "def qa_pipeline(\n",
    "    squad_url: str = \"http://github.com/sciling/qatransfer/releases/download/v0.1/save.zip\",\n",
    "    squad_load_path: str = \"/save/out/squad/basic/00/save/basic-2000\",\n",
    "    squad_shared_path: str = \"/save/out/squad/basic/00/shared.json\",\n",
    "    train_run_id: str = \"00\",\n",
    "    train_sent_size_th: str = \"150\",\n",
    "    train_ques_size_th: str = \"100\",\n",
    "    train_num_epochs: str = \"12\",\n",
    "    train_num_steps: str = \"5000\",\n",
    "    train_eval_period: str = \"200\",\n",
    "    train_save_period: str = \"200\",\n",
    "    test_start_step: int = 2001,\n",
    "    test_end_step: int = 2201,\n",
    "    test_eval_period: int = 200,\n",
    "    test_threshold: float = 0.5,\n",
    "    test_th: int = 10,\n",
    "    test_reranking_th: int = 10,\n",
    "    test_format: str = \"trec\",\n",
    "    test_verbose: bool = False,\n",
    "    test_ignore_noanswer: bool = False,\n",
    "):\n",
    "\n",
    "    # Download\n",
    "    dataset_path = download_op(squad_url)\n",
    "\n",
    "    # Preprocess semeval\n",
    "    semeval_prepro = semeval_prepro_op(dataset_path.output)\n",
    "\n",
    "    # Train semeval with pretrained model SQUAD\n",
    "    semeval_model = semeval_train_op(\n",
    "        dataset_path.output,\n",
    "        semeval_prepro.output,\n",
    "        load_path=squad_load_path,\n",
    "        shared_path=squad_shared_path,\n",
    "        run_id=train_run_id,\n",
    "        sent_size_th=train_sent_size_th,\n",
    "        ques_size_th=train_ques_size_th,\n",
    "        num_epochs=train_num_epochs,\n",
    "        num_steps=train_num_steps,\n",
    "        eval_period=train_eval_period,\n",
    "        save_period=train_save_period,\n",
    "    )\n",
    "\n",
    "    # Generate files for testing\n",
    "    test_files = semeval_generate_test_files_op(\n",
    "        semeval_prepro.output,\n",
    "        semeval_model.output,\n",
    "        test_start_step,\n",
    "        test_end_step,\n",
    "        test_eval_period,\n",
    "        train_run_id,\n",
    "        test_threshold,\n",
    "    )\n",
    "\n",
    "    # Test\n",
    "    semeval_test_op(\n",
    "        test_files.output,\n",
    "        train_run_id,\n",
    "        test_start_step,\n",
    "        test_end_step,\n",
    "        test_th,\n",
    "        test_reranking_th,\n",
    "        test_format,\n",
    "        test_verbose,\n",
    "        test_ignore_noanswer,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Connect with AWS Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "# Disable ssl verification\n",
    "from kfp_server_api.configuration import Configuration as Configuration\n",
    "if 'old_init' not in globals():\n",
    "    old_init = Configuration.__init__\n",
    "def new_init(self, *k, **kw):\n",
    "    old_init(self, *k, **kw)\n",
    "    self.verify_ssl = False\n",
    "Configuration.__init__ = new_init\n",
    "cookies = \"authservice_session=YOUR_TOKEN\"\n",
    "client = kfp.Client(host='http://istio-ingressgateway.istio-system.svc/pipeline', namespace='admin', cookies=cookies)\n",
    "client.list_experiments(namespace=\"admin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = qa_pipeline\n",
    "experiment_name = 'semeval'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  '{}.zip'.format(experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Define arguments and create a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGUMENTS DEFINITION\n",
    "arguments = {\n",
    "    'train_sent_size_th' : '10',\n",
    "    'train_ques_size_th':'10', \n",
    "    \"train_num_epochs\" : '1',\n",
    "    \"train_num_steps\" : '3',\n",
    "    \"train_eval_period\" : '1',\n",
    "    \"train_save_period\" : '1',\n",
    "    \"test_start_step\" : 2001, \n",
    "    \"test_end_step\" : 2004,\n",
    "}\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  namespace='admin',\n",
    "                                                  arguments=arguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
