{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiQA Question Answering through Transfer Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install necessary packages and restart de kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip requirements\n",
    "!python -m pip install --user --upgrade pip\n",
    "!pip install --user --upgrade kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow SDK\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import OutputPath, InputPath, func_to_container_op, OutputBinaryFile, InputBinaryFile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(squad_url, dataset_path: OutputPath()):\n",
    "    import os\n",
    "    import tempfile\n",
    "    import zipfile\n",
    "\n",
    "    import requests\n",
    "\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Download WikiQA\n",
    "    r = requests.get(\n",
    "        \"https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\",\n",
    "        stream=True,\n",
    "    )\n",
    "    total_size_in_bytes = int(r.headers.get(\"content-length\", 0))\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n",
    "    with tempfile.TemporaryFile() as tf:\n",
    "        for chunk in r.iter_content(chunk_size=128):\n",
    "            progress_bar.update(len(chunk))\n",
    "            tf.write(chunk)\n",
    "        with zipfile.ZipFile(tf, \"r\") as f:\n",
    "            f.extractall(dataset_path)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "\n",
    "    # Download GloVe\n",
    "    GLOVE_DIR = dataset_path + \"/glove\"\n",
    "    os.makedirs(GLOVE_DIR, exist_ok=True)\n",
    "    r = requests.get(\"http://nlp.stanford.edu/data/glove.6B.zip\", stream=True)\n",
    "    total_size_in_bytes = int(r.headers.get(\"content-length\", 0))\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n",
    "    with tempfile.TemporaryFile() as tf:\n",
    "        for chunk in r.iter_content(chunk_size=1024):\n",
    "            progress_bar.update(len(chunk))\n",
    "            tf.write(chunk)\n",
    "        with zipfile.ZipFile(tf, \"r\") as f:\n",
    "            f.extractall(GLOVE_DIR)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "\n",
    "    # Download Squad\n",
    "    r_squad = requests.get(squad_url)\n",
    "    total_size_in_bytes = int(r_squad.headers.get(\"content-length\", 0))\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n",
    "    with tempfile.TemporaryFile() as tf:\n",
    "        for chunk in r_squad.iter_content(chunk_size=1024):\n",
    "            progress_bar.update(len(chunk))\n",
    "            tf.write(chunk)\n",
    "        with zipfile.ZipFile(tf, \"r\") as f:\n",
    "            f.extractall(dataset_path)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "\n",
    "    print(os.listdir(dataset_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess wikiqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_class(dataset_path: InputPath(str), wikiqa_path: OutputPath(str)):\n",
    "    import nltk\n",
    "\n",
    "    from wikiqa.prepro_class import prepro\n",
    "\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "    def get_args():\n",
    "        from types import SimpleNamespace\n",
    "\n",
    "        source_dir = dataset_path + \"/WikiQACorpus\"\n",
    "        target_dir = wikiqa_path + \"/wikiqa-class\"\n",
    "        glove_dir = dataset_path + \"/glove\"\n",
    "        args = SimpleNamespace(\n",
    "            source_dir=source_dir,\n",
    "            target_dir=target_dir,\n",
    "            debug=False,\n",
    "            glove_corpus=\"6B\",\n",
    "            glove_dir=glove_dir,\n",
    "            glove_vec_size=\"100\",\n",
    "            tokenizer=\"PTB\",\n",
    "        )\n",
    "        return args\n",
    "\n",
    "    args = get_args()\n",
    "    prepro(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. WikiQA train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikiqa_train(\n",
    "    dataset_path: InputPath(str),\n",
    "    wikiqa_path: InputPath(str),\n",
    "    load_path,\n",
    "    shared_path,\n",
    "    run_id,\n",
    "    sent_size_th,\n",
    "    ques_size_th,\n",
    "    num_epochs,\n",
    "    num_steps,\n",
    "    eval_period,\n",
    "    save_period,\n",
    "    model_path: OutputPath(str),\n",
    "):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    from basic.cli import main\n",
    "\n",
    "    input_dir = wikiqa_path + \"/wikiqa-class\"\n",
    "    output_dir = model_path + \"/out/wikiqa\"\n",
    "    full_load_path = dataset_path + load_path\n",
    "    full_shared_path = dataset_path + shared_path\n",
    "    tf.app.run(\n",
    "        main,\n",
    "        argv=[\n",
    "            \"./basic/cli.py\",\n",
    "            \"--data_dir\",\n",
    "            input_dir,\n",
    "            \"--out_base_dir\",\n",
    "            output_dir,\n",
    "            \"--load_path\",\n",
    "            full_load_path,\n",
    "            \"--shared_path\",\n",
    "            full_shared_path,\n",
    "            \"--load_trained_model\",\n",
    "            \"--run_id\",\n",
    "            run_id,\n",
    "            \"--sent_size_th\",\n",
    "            sent_size_th,\n",
    "            \"--ques_size_th\",\n",
    "            ques_size_th,\n",
    "            \"--num_epochs\",\n",
    "            num_epochs,\n",
    "            \"--num_steps\",\n",
    "            num_steps,\n",
    "            \"--eval_period\",\n",
    "            eval_period,\n",
    "            \"--save_period\",\n",
    "            save_period,\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. WikiQA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikiqa_test(\n",
    "    dataset_path: InputPath(str),\n",
    "    wikiqa_path: InputPath(str),\n",
    "    prev_model_path: InputPath(str),\n",
    "    shared_path,\n",
    "    run_id,\n",
    "    sent_size_th,\n",
    "    ques_size_th,\n",
    "    num_epochs,\n",
    "    num_steps,\n",
    "    eval_period,\n",
    "    save_period,\n",
    "    mlpipeline_metrics_path: OutputPath(\"Metrics\"),\n",
    "    model_path: OutputPath(str),\n",
    "):\n",
    "    input_dir = wikiqa_path + \"/wikiqa-class\"\n",
    "    output_dir = model_path + \"/out/wikiqa\"\n",
    "\n",
    "    import shutil\n",
    "\n",
    "    src = prev_model_path + \"/out/wikiqa\"\n",
    "    dst = model_path + \"/out/wikiqa\"\n",
    "    shutil.copytree(src, dst)\n",
    "\n",
    "    full_shared_path = dataset_path + shared_path\n",
    "    import os\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    flags = tf.app.flags\n",
    "\n",
    "    # Names and directories\n",
    "    flags.DEFINE_string(\"model_name\", \"basic-class\", \"Model name [basic | basic-class]\")\n",
    "    flags.DEFINE_string(\"data_dir\", input_dir, \"Data dir [data/squad]\")\n",
    "    flags.DEFINE_string(\"run_id\", run_id, \"Run ID [0]\")\n",
    "    flags.DEFINE_string(\"out_base_dir\", output_dir, \"out base dir [out]\")\n",
    "    flags.DEFINE_string(\"forward_name\", \"single\", \"Forward name [single]\")\n",
    "    flags.DEFINE_string(\"answer_path\", \"\", \"Answer path []\")\n",
    "    flags.DEFINE_string(\"eval_path\", \"\", \"Eval path []\")\n",
    "    flags.DEFINE_string(\"load_path\", \"\", \"Load path []\")\n",
    "    flags.DEFINE_string(\"shared_path\", full_shared_path, \"Shared path []\")\n",
    "\n",
    "    # Device placement\n",
    "    flags.DEFINE_string(\n",
    "        \"device\", \"/cpu:0\", \"default device for summing gradients. [/cpu:0]\"\n",
    "    )\n",
    "    flags.DEFINE_string(\n",
    "        \"device_type\",\n",
    "        \"gpu\",\n",
    "        \"device for computing gradients (parallelization). cpu | gpu [gpu]\",\n",
    "    )\n",
    "    flags.DEFINE_integer(\n",
    "        \"num_gpus\", 1, \"num of gpus or cpus for computing gradients [1]\"\n",
    "    )\n",
    "\n",
    "    # Essential training and test options\n",
    "    flags.DEFINE_string(\"mode\", \"test\", \"train | test | forward [test]\")\n",
    "    flags.DEFINE_boolean(\"load\", True, \"load saved data? [True]\")\n",
    "    flags.DEFINE_bool(\"single\", False, \"supervise only the answer sentence? [False]\")\n",
    "    flags.DEFINE_boolean(\"debug\", False, \"Debugging mode? [False]\")\n",
    "    flags.DEFINE_bool(\n",
    "        \"load_ema\", True, \"load exponential average of variables when testing?  [True]\"\n",
    "    )\n",
    "    flags.DEFINE_bool(\"eval\", True, \"eval? [True]\")\n",
    "    flags.DEFINE_bool(\"train_only_output\", False, \"Train only output module?\")\n",
    "    flags.DEFINE_bool(\"load_trained_model\", False, \"Load SQUAD trained model\")\n",
    "    flags.DEFINE_bool(\"freeze_phrase_layer\", False, \"Freeze phrase layer\")\n",
    "    flags.DEFINE_bool(\"freeze_att_layer\", False, \"Freeze att layer\")\n",
    "    flags.DEFINE_bool(\n",
    "        \"freeze_span_modelling_layer\", False, \"Freeze modelling layer for span\"\n",
    "    )\n",
    "\n",
    "    flags.DEFINE_bool(\"using_shared\", False, \"using pre-created shared.json\")\n",
    "    flags.DEFINE_bool(\"load_shared\", False, \"load shared.json for each batch\")\n",
    "    flags.DEFINE_string(\"dev_name\", \"test\", \"using dev or test?\")\n",
    "    flags.DEFINE_string(\"test_name\", \"test\", \"using test or dev?\")\n",
    "\n",
    "    # Training / test parameters\n",
    "    flags.DEFINE_integer(\"batch_size\", 60, \"Batch size [60]\")\n",
    "    flags.DEFINE_integer(\"val_num_batches\", 100, \"validation num batches [100]\")\n",
    "    flags.DEFINE_integer(\"test_num_batches\", 0, \"test num batches [0]\")\n",
    "    flags.DEFINE_integer(\n",
    "        \"num_epochs\", int(num_epochs), \"Total number of epochs for training [12]\"\n",
    "    )\n",
    "    flags.DEFINE_integer(\"num_steps\", int(num_steps), \"Number of steps [20000]\")\n",
    "    flags.DEFINE_integer(\"load_step\", 0, \"load step [0]\")\n",
    "    flags.DEFINE_float(\"init_lr\", 0.5, \"Initial learning rate [0.5]\")\n",
    "    flags.DEFINE_float(\n",
    "        \"input_keep_prob\", 0.8, \"Input keep prob for the dropout of LSTM weights [0.8]\"\n",
    "    )\n",
    "    flags.DEFINE_float(\n",
    "        \"keep_prob\", 0.8, \"Keep prob for the dropout of Char-CNN weights [0.8]\"\n",
    "    )\n",
    "    flags.DEFINE_float(\"wd\", 0.0, \"L2 weight decay for regularization [0.0]\")\n",
    "    flags.DEFINE_integer(\"hidden_size\", 100, \"Hidden size [100]\")\n",
    "    flags.DEFINE_integer(\"char_out_size\", 100, \"char-level word embedding size [100]\")\n",
    "    flags.DEFINE_integer(\"char_emb_size\", 8, \"Char emb size [8]\")\n",
    "    flags.DEFINE_string(\n",
    "        \"out_channel_dims\",\n",
    "        \"100\",\n",
    "        \"Out channel dims of Char-CNN, separated by commas [100]\",\n",
    "    )\n",
    "    flags.DEFINE_string(\n",
    "        \"filter_heights\", \"5\", \"Filter heights of Char-CNN, separated by commas [5]\"\n",
    "    )\n",
    "    flags.DEFINE_bool(\"finetune\", False, \"Finetune word embeddings? [False]\")\n",
    "    flags.DEFINE_bool(\"highway\", True, \"Use highway? [True]\")\n",
    "    flags.DEFINE_integer(\"highway_num_layers\", 2, \"highway num layers [2]\")\n",
    "    flags.DEFINE_bool(\"share_cnn_weights\", True, \"Share Char-CNN weights [True]\")\n",
    "    flags.DEFINE_bool(\n",
    "        \"share_lstm_weights\",\n",
    "        True,\n",
    "        \"Share pre-processing (phrase-level) LSTM weights [True]\",\n",
    "    )\n",
    "    flags.DEFINE_float(\n",
    "        \"var_decay\", 0.999, \"Exponential moving average decay for variables [0.999]\"\n",
    "    )\n",
    "    flags.DEFINE_string(\"classifier\", \"maxpool\", \"[maxpool, sumpool, default]\")\n",
    "\n",
    "    # Optimizations\n",
    "    flags.DEFINE_bool(\"cluster\", True, \"Cluster data for faster training [False]\")\n",
    "    flags.DEFINE_bool(\"len_opt\", True, \"Length optimization? [False]\")\n",
    "    flags.DEFINE_bool(\n",
    "        \"cpu_opt\", False, \"CPU optimization? GPU computation can be slower [False]\"\n",
    "    )\n",
    "\n",
    "    # Logging and saving options\n",
    "    flags.DEFINE_boolean(\"progress\", True, \"Show progress? [True]\")\n",
    "    flags.DEFINE_integer(\"log_period\", 100, \"Log period [100]\")\n",
    "    flags.DEFINE_integer(\"eval_period\", int(eval_period), \"Eval period [1000]\")\n",
    "    flags.DEFINE_integer(\"save_period\", int(save_period), \"Save Period [1000]\")\n",
    "    flags.DEFINE_integer(\"max_to_keep\", 20, \"Max recent saves to keep [20]\")\n",
    "    flags.DEFINE_bool(\"dump_eval\", True, \"dump eval? [True]\")\n",
    "    flags.DEFINE_bool(\"dump_answer\", False, \"dump answer? [True]\")\n",
    "    flags.DEFINE_bool(\"vis\", False, \"output visualization numbers? [False]\")\n",
    "    flags.DEFINE_bool(\"dump_pickle\", True, \"Dump pickle instead of json? [True]\")\n",
    "    flags.DEFINE_float(\n",
    "        \"decay\", 0.9, \"Exponential moving average decay for logging values [0.9]\"\n",
    "    )\n",
    "\n",
    "    # Thresholds for speed and less memory usage\n",
    "    flags.DEFINE_integer(\"word_count_th\", 10, \"word count th [100]\")\n",
    "    flags.DEFINE_integer(\"char_count_th\", 50, \"char count th [500]\")\n",
    "    flags.DEFINE_integer(\"sent_size_th\", int(sent_size_th), \"sent size th [64]\")\n",
    "    flags.DEFINE_integer(\"num_sents_th\", 1, \"num sents th [8]\")\n",
    "    flags.DEFINE_integer(\"ques_size_th\", int(ques_size_th), \"ques size th [32]\")\n",
    "    flags.DEFINE_integer(\"word_size_th\", 16, \"word size th [16]\")\n",
    "    flags.DEFINE_integer(\"para_size_th\", 256, \"para size th [256]\")\n",
    "\n",
    "    # Advanced training options\n",
    "    flags.DEFINE_bool(\"lower_word\", True, \"lower word [True]\")\n",
    "    flags.DEFINE_bool(\"squash\", False, \"squash the sentences into one? [False]\")\n",
    "    flags.DEFINE_bool(\"swap_memory\", True, \"swap memory? [True]\")\n",
    "    flags.DEFINE_string(\"data_filter\", \"max\", \"max | valid | semi [max]\")\n",
    "    flags.DEFINE_bool(\"use_glove_for_unk\", True, \"use glove for unk [False]\")\n",
    "    flags.DEFINE_bool(\n",
    "        \"known_if_glove\", True, \"consider as known if present in glove [False]\"\n",
    "    )\n",
    "    flags.DEFINE_string(\"logit_func\", \"tri_linear\", \"logit func [tri_linear]\")\n",
    "    flags.DEFINE_string(\"answer_func\", \"linear\", \"answer logit func [linear]\")\n",
    "    flags.DEFINE_string(\"sh_logit_func\", \"tri_linear\", \"sh logit func [tri_linear]\")\n",
    "\n",
    "    # Ablation options\n",
    "    flags.DEFINE_bool(\"use_char_emb\", True, \"use char emb? [True]\")\n",
    "    flags.DEFINE_bool(\"use_word_emb\", True, \"use word embedding? [True]\")\n",
    "    flags.DEFINE_bool(\"q2c_att\", True, \"question-to-context attention? [True]\")\n",
    "    flags.DEFINE_bool(\"c2q_att\", True, \"context-to-question attention? [True]\")\n",
    "    flags.DEFINE_bool(\"dynamic_att\", False, \"Dynamic attention [False]\")\n",
    "\n",
    "    def main(_):\n",
    "        from basic.main import main as m\n",
    "\n",
    "        config = flags.FLAGS\n",
    "        config.model_name = \"basic-class\"\n",
    "        config.out_dir = os.path.join(\n",
    "            config.out_base_dir, config.model_name, str(config.run_id).zfill(2)\n",
    "        )\n",
    "\n",
    "        print(config.out_dir)\n",
    "        evaluator = m(config)\n",
    "\n",
    "        \"\"\"Generating metrics for the squad model\"\"\"\n",
    "        metrics = {\n",
    "            \"metrics\": [\n",
    "                {\n",
    "                    \"name\": \"accuracy-score\",\n",
    "                    \"numberValue\": str(evaluator.acc),\n",
    "                    \"format\": \"RAW\",\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"loss\",\n",
    "                    \"numberValue\": str(evaluator.loss),\n",
    "                    \"format\": \"RAW\",\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        import json\n",
    "\n",
    "        with open(mlpipeline_metrics_path, \"w\") as f:\n",
    "            json.dump(metrics, f)\n",
    "\n",
    "    tf.app.run(main)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. WikiQA evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikiqa_evaluate(\n",
    "    wikiqa_path: InputPath(str),\n",
    "    model_path: InputPath(str),\n",
    "    start_step,\n",
    "    end_step,\n",
    "    eval_period,\n",
    "    run_ids,\n",
    "    mlpipeline_metrics_path: OutputPath(\"Metrics\"),\n",
    "):\n",
    "    from wikiqa.result import evaluate\n",
    "    from wikiqa.result import load\n",
    "\n",
    "    end_step = int(end_step)\n",
    "    start_step = int(start_step)\n",
    "    eval_period = int(eval_period)\n",
    "\n",
    "    def get_args():\n",
    "        from types import SimpleNamespace\n",
    "\n",
    "        data_dir = wikiqa_path + \"/wikiqa-class\"\n",
    "        eval_dir = model_path + \"/out/wikiqa/basic-class\"\n",
    "        args = SimpleNamespace(\n",
    "            data_dir=data_dir,\n",
    "            eval_dir=eval_dir,\n",
    "            run_ids=run_ids,\n",
    "            eval_name=\"test\",\n",
    "            eval_period=eval_period,\n",
    "            start_step=start_step,\n",
    "            end_step=end_step,\n",
    "            steps=\"\",\n",
    "            ensemble=False,\n",
    "        )\n",
    "        return args\n",
    "\n",
    "    def main():\n",
    "        metrics_list = []\n",
    "        args = get_args()\n",
    "        data = load(args)\n",
    "        for run_id in args.run_ids.split(\",\"):\n",
    "            best_eval, best_global_step = (0, 0, 0), -1\n",
    "            print(\"Evaluate run_id = %s...\" % run_id)\n",
    "            for global_step in range(\n",
    "                args.start_step, args.end_step + args.eval_period, args.eval_period\n",
    "            ):\n",
    "                curr_eval = evaluate(args, [run_id], data, [global_step])\n",
    "                if curr_eval[0] > best_eval[0]:\n",
    "                    best_eval, best_global_step = curr_eval, global_step\n",
    "            print(\n",
    "                \"Best MAP: %.2f\\tMRR: %.2f\\tP@1: %.2f in global step %d\"\n",
    "                % (best_eval[0], best_eval[1], best_eval[2], best_global_step)\n",
    "            )\n",
    "\n",
    "            \"\"\"Generating metrics for the squad model\"\"\"\n",
    "            metrics_list.append(\n",
    "                {\n",
    "                    \"name\": \"MAP_for_run_%d\" % best_global_step,\n",
    "                    \"numberValue\": str(best_eval[0]),\n",
    "                    \"format\": \"RAW\",\n",
    "                }\n",
    "            )\n",
    "            metrics_list.append(\n",
    "                {\n",
    "                    \"name\": \"MRR_for_run_%d\" % best_global_step,\n",
    "                    \"numberValue\": str(best_eval[1]),\n",
    "                    \"format\": \"RAW\",\n",
    "                }\n",
    "            )\n",
    "            metrics_list.append(\n",
    "                {\n",
    "                    \"name\": \"P1_for_run_%d\" % best_global_step,\n",
    "                    \"numberValue\": str(best_eval[2]),\n",
    "                    \"format\": \"RAW\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        metrics = {\"metrics\": metrics_list}\n",
    "\n",
    "        import json\n",
    "\n",
    "        with open(mlpipeline_metrics_path, \"w\") as f:\n",
    "            json.dump(metrics, f)\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transform python functions into components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_op = func_to_container_op(\n",
    "    download,\n",
    "    base_image=\"tensorflow/tensorflow:latest-gpu-py3\",\n",
    "    packages_to_install=[\"tqdm\"],\n",
    ")\n",
    "\n",
    "wikiqa_prepro_op = func_to_container_op(\n",
    "    prepro_class,\n",
    "    base_image=\"sciling/tensorflow:0.12.0-py3\",\n",
    "    packages_to_install=[\n",
    "        \"https://github.com/sciling/qatransfer/archive/refs/heads/master.zip#egg=qatransfer\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "train_op = func_to_container_op(\n",
    "    wikiqa_train,\n",
    "    base_image=\"sciling/tensorflow:0.12.0-py3\",\n",
    "    packages_to_install=[\n",
    "        \"https://github.com/sciling/qatransfer/archive/refs/heads/master.zip#egg=qatransfer\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "evaluate_op = func_to_container_op(\n",
    "    wikiqa_evaluate,\n",
    "    base_image=\"sciling/tensorflow:0.12.0-py3\",\n",
    "    packages_to_install=[\n",
    "        \"https://github.com/sciling/qatransfer/archive/refs/heads/master.zip#egg=qatransfer\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "test_op = func_to_container_op(\n",
    "    wikiqa_test,\n",
    "    base_image=\"sciling/tensorflow:0.12.0-py3\",\n",
    "    packages_to_install=[\n",
    "        \"https://github.com/sciling/qatransfer/archive/refs/heads/master.zip#egg=qatransfer\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define pipeline and component connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_pipeline(\n",
    "    squad_url: str = \"https://github.com/sciling/qatransfer/releases/download/v0.1/save_class.zip\",\n",
    "    squad_load_path: str = \"/save/out/squad/basic-class/00/save/basic-class-1\",\n",
    "    squad_shared_path: str = \"/save/out/squad/basic-class/00/shared.json\",\n",
    "    run_id: str = \"00\",\n",
    "    sent_size_th: str = \"500\",\n",
    "    ques_size_th: str = \"30\",\n",
    "    num_epochs: str = \"12\",\n",
    "    num_steps: str = \"5000\",\n",
    "    eval_period: str = \"200\",\n",
    "    save_period: str = \"200\",\n",
    "    start_step: int = 2001,\n",
    "    end_step: int = 2201,\n",
    "):\n",
    "    # Download\n",
    "    dataset_path = download_op(squad_url)\n",
    "\n",
    "    # Preprocess wikiqa\n",
    "    wikiqa_prepro = wikiqa_prepro_op(dataset_path.output)\n",
    "\n",
    "    # Train wikiqa with pretrained model SQUAD\n",
    "    trained_model = train_op(\n",
    "        dataset_path.output,\n",
    "        wikiqa_prepro.output,\n",
    "        load_path=squad_load_path,\n",
    "        shared_path=squad_shared_path,\n",
    "        run_id=run_id,\n",
    "        sent_size_th=sent_size_th,\n",
    "        ques_size_th=ques_size_th,\n",
    "        num_epochs=num_epochs,\n",
    "        num_steps=num_steps,\n",
    "        eval_period=eval_period,\n",
    "        save_period=save_period,\n",
    "    )\n",
    "\n",
    "    evaluate_op(\n",
    "        wikiqa_prepro.output,\n",
    "        trained_model.output,\n",
    "        start_step,\n",
    "        end_step,\n",
    "        eval_period,\n",
    "        run_id,\n",
    "    )\n",
    "\n",
    "    test_op(\n",
    "        dataset_path.output,\n",
    "        wikiqa_prepro.output,\n",
    "        trained_model.output,\n",
    "        shared_path=squad_shared_path,\n",
    "        run_id=run_id,\n",
    "        sent_size_th=sent_size_th,\n",
    "        ques_size_th=ques_size_th,\n",
    "        num_epochs=num_epochs,\n",
    "        num_steps=num_steps,\n",
    "        eval_period=eval_period,\n",
    "        save_period=save_period,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Connect with AWS Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "# Disable ssl verification\n",
    "from kfp_server_api.configuration import Configuration as Configuration\n",
    "if 'old_init' not in globals():\n",
    "    old_init = Configuration.__init__\n",
    "def new_init(self, *k, **kw):\n",
    "    old_init(self, *k, **kw)\n",
    "    self.verify_ssl = False\n",
    "Configuration.__init__ = new_init\n",
    "cookies = \"YOUR TOKEN\"\n",
    "client = kfp.Client(host='http://istio-ingressgateway.istio-system.svc/pipeline', namespace='admin', cookies=cookies)\n",
    "client.list_experiments(namespace=\"admin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = qa_pipeline\n",
    "experiment_name = 'wikiqa'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  '{}.zip'.format(experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Define arguments and create a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGUMENTS DEFINITION\n",
    "arguments = {\n",
    "    'sent_size_th' : '10',\n",
    "    'ques_size_th':'10', \n",
    "    \"num_epochs\" : '1',\n",
    "    \"num_steps\" : '3',\n",
    "    \"eval_period\" : '1',\n",
    "    \"save_period\" : '1',\n",
    "    \"start_step\" : 2, \n",
    "    \"end_step\" : 2,\n",
    "}\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  namespace='admin',\n",
    "                                                  arguments=arguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
